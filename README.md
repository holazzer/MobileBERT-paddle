# MobileBERT paddle

ä½¿ç”¨ `paddlepaddle` å®ç° `transformers` ä¸­æä¾›çš„ `MobileBERT` æ¨¡å‹ã€‚


è®ºæ–‡ï¼š **MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices**

https://arxiv.org/abs/2004.02984


huggingface æ¨¡å‹é¡µé¢ï¼š  

https://huggingface.co/google/mobilebert-uncased


transformers æºä»£ç ï¼š

https://github.com/huggingface/transformers/tree/master/src/transformers/models/mobilebert


## è®ºæ–‡å’Œä»£ç è§£æ

æœ¬æ–‡å¯¹bertæ¨¡å‹è¿›è¡Œäº†çŸ¥è¯†è¿ç§»ï¼ŒæŠŠå¤§æ¨¡å‹çš„çŸ¥è¯†è¿ç§»åˆ°å°æ¨¡å‹ä¸­ã€‚

æ·±å±‚ç½‘ç»œä¼šå¾ˆéš¾è®­ç»ƒï¼Œå°¤å…¶æ˜¯åœ¨å°æ¨¡å‹ä¸­æˆ‘ä»¬æŠŠæ¨¡å‹çš„â€œè…°â€æ”¶çš„å¾ˆç´§ï¼Œè¿™æ ·å°±æ›´ä¸å®¹æ˜“è®­ç»ƒäº†ã€‚æ‰€ä»¥è¿™é‡Œä½œè€…é‡‡å–çš„æ–¹æ³•æ˜¯ï¼Œå…ˆè®­ç»ƒä¸€ä¸ªå¤§å°ºå¯¸çš„ç½‘ç»œä½œä¸ºæ•™å¸ˆï¼Œç„¶ååœ¨å°æ¨¡å‹ï¼ˆå­¦ç”Ÿï¼‰ç½‘ç»œçš„è®¾è®¡ä¸­ï¼ŒæŠŠæ¯ä¸€å±‚çš„ feature map è®¾è®¡æˆç›¸åŒçš„å½¢çŠ¶ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥åœ¨è®­ç»ƒæ—¶ï¼Œè®©è¿™ä¸¤ä¸ªæ¨¡å‹å°½é‡å¯¹é½ã€‚

### æ¨¡å‹è®¾è®¡

è¿™é‡Œç»“åˆè®ºæ–‡å’Œä»£ç ï¼Œå¯¹æ¨¡å‹è®¾è®¡è¿›è¡Œä»‹ç»ã€‚æˆ‘æƒ³ç”¨ä¸€ä¸ªå…ˆæ€»ååˆ†çš„æ–¹æ³•æ¥è®²ã€‚

å…ˆè‡ªä¸Šè€Œä¸‹ï¼Œè®²è§£æ¨¡å‹çš„æ„é€ ï¼Œå†ä»ä¸‹åˆ°ä¸Šï¼Œçœ‹æ¯ä¸€ä¸ªå­ç½‘ç»œçš„å®ç°ã€‚

```python
class MobileBertModel(MobileBertPreTrainedModel):
    def __init__(self, config, add_pooling_layer=True):
        super().__init__(config)
        self.config = config
        self.embeddings = MobileBertEmbeddings(config)
        self.encoder = MobileBertEncoder(config)
        self.pooler = MobileBertPooler(config) if add_pooling_layer else None
```

`MobileBertModel` åŒ…æ‹¬ embedding å’Œ encoderï¼Œä»¥åŠä¸€ä¸ª poolerã€‚
ï¼ˆåŸæ–‡ä¸­æ²¡æœ‰ poolerï¼Œ æ‰€ä»¥æˆ‘ä»¬ä¸€ä¼šå„¿å…ˆçœ‹çœ‹ pooleræ˜¯å¹²ä»€ä¹ˆçš„ã€‚ï¼‰

![](static/table_1.png)




### ğŸŒŠ Pooler

æˆ‘ä»¬å…ˆçœ‹è¿™ä¸ªâ€œå¯æœ‰å¯æ— â€çš„ pooler:

```python
class MobileBertPooler(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.do_activate = config.classifier_activation
        if self.do_activate:
            self.dense = nn.Linear(512, 512)

    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        if not self.do_activate:
            return first_token_tensor
        else:
            pooled_output = self.dense(first_token_tensor)
            pooled_output = paddle.tanh(pooled_output)
            return pooled_output
```

è§£é‡Šï¼šencoderæœ€åä¼šè¾“å‡º `(batch_size, 24, 512)` ç»´çš„å‘é‡ã€‚

24æ˜¯bodyçš„æ•°é‡ï¼Œ512æ˜¯è®¾ç½®çš„embeddingç»´åº¦ï¼ŒMobileBERTçš„bodyè®¾è®¡åˆšå¥½æ˜¯è¿›512å‡º512ã€‚

è¿™é‡Œpoolerçš„æ„æ€æ˜¯ç›´æ¥æ‹¿ç¬¬ä¸€ä¸ª512ä½œä¸ºæ¨¡å‹è¾“å‡ºï¼Œæˆ–è€…æ˜¯å†åŠ ä¸€å±‚ Linearï¼Œè¿˜æ˜¯è¾“å‡º 512ã€‚ 

å› æ­¤ï¼Œè¿™ä¸ª Pooler ç¡®å®æ˜¯ â€œå¯æœ‰å¯æ— â€ï¼Œ å“ˆå“ˆã€‚


### ğŸ© Embedding

```python
class MobileBertEmbeddings(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.trigram_input = True
        self.embedding_size = 128
        self.hidden_size = 512

        self.word_embeddings = nn.Embedding(config.vocab_size, 128, padding_idx=0)
        self.position_embeddings = nn.Embedding(512, 512)
        self.token_type_embeddings = nn.Embedding(2, 512)

        embed_dim_multiplier = 3 if self.trigram_input else 1
        embedded_input_size = self.embedding_size * embed_dim_multiplier
        self.embedding_transformation = nn.Linear(embedded_input_size, 512)

        self.LayerNorm = NoNorm(512)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
```

ï¼ˆå…¶ä¸­éƒ¨åˆ† config é¡¹è¢«æˆ‘æ¢æˆäº†æ•°å­—ã€‚ï¼‰
å¾ˆæ¸…æ¥šï¼Œä¸€ä¸ªword embeddingï¼Œä¸€ä¸ª position embeddingï¼Œä¸€ä¸ª token type embeddingã€‚

```python
    def forward(self, input_ids, token_type_ids, position_ids, inputs_embeds):
        if self.trigram_input:
            inputs_embeds = paddle.concat([
                nn.functional.pad(inputs_embeds[:, 1:], [0, 0, 0, 1, 0, 0], value=0),
                inputs_embeds,
                nn.functional.pad(inputs_embeds[:, :-1], [0, 0, 1, 0, 0, 0], value=0),
            ], axis=2)

        if self.trigram_input or self.embedding_size != self.hidden_size:
            inputs_embeds = self.embedding_transformation(inputs_embeds)

        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(token_type_ids)
        
        embeddings = inputs_embeds + position_embeddings + token_type_embeddings

        embeddings = self.LayerNorm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings
```

ï¼ˆä¸ºæ–¹ä¾¿é˜…è¯»ï¼Œéƒ¨åˆ†åˆ å»ã€‚ï¼‰

ç”¨3-gramçš„è¯ï¼Œå°±è¦æŠŠ input å·¦é”™å¼€1ä½ï¼Œå³é”™å¼€1ä½ï¼Œconcatèµ·æ¥ï¼Œå°±å¾—åˆ°äº†3-gramå‘é‡ã€‚

å†è¿‡ä¸€ä¸ª embedding å˜æˆ512ã€‚MobileBERTçš„ä¸€ä¸ªè®¾è®¡å°±æ˜¯è¾“å…¥çš„embeddingå°ºå¯¸æ˜¯128ï¼Œç»è¿‡ä¸€ä¸ªfcå˜æˆ512ã€‚


### ğŸ­ Encoder

Encoder æ˜¯æ¨¡å‹çš„ä¸»å¹²éƒ¨åˆ†ã€‚

```python
class MobileBertEncoder(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.layer = nn.LayerList(
            [MobileBertLayer(config) for _ in range(24)])
```

è®ºæ–‡ä¸­çš„å›¾æœç„¶è¯šä¸æ¬ºæˆ‘ï¼ŒçœŸçš„å°±æ˜¯24ä¸ªbodyçš„éƒ¨åˆ†ä¸²èµ·æ¥ã€‚

```python
    def forward(
            self,
            hidden_states,
            attention_mask=None,
            head_mask=None,
            output_attentions=False,     # æ˜¯å¦è¦è¾“å‡º attention
            output_hidden_states=False,  # æ˜¯å¦è¦è¾“å‡ºéšè—çŠ¶æ€
            return_dict=True,
    ):
        all_hidden_states = () if output_hidden_states else None
        all_attentions = () if output_attentions else None



        for i, layer_module in enumerate(self.layer):  # æ¯æ¬¡è¿‡ä¸€ä¸ªlayer
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_outputs = layer_module(
                hidden_states,
                attention_mask,
                head_mask[i],
                output_attentions,
            )

            hidden_states = layer_outputs[0]  # <-  éšè—çŠ¶æ€ç”¨äºä¸‹ä¸€å±‚è¾“å…¥

            if output_attentions:
                all_attentions = all_attentions + (layer_outputs[1],)  # <- æœ¬å±‚ attention 

        # Add last layer
        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if
                         v is not None)
        return BaseModelOutput(last_hidden_state=hidden_states,
                               hidden_states=all_hidden_states,
                               attentions=all_attentions)
```

å¾ˆç›´è§‚ï¼Œå°±æ˜¯æŠŠ24ä¸ª Layer å±‚è¿‡äº†ä¸€éï¼Œæ¯ä¸€å±‚ä¼šè¾“å‡ºè‡ªå·±çš„ hidden stateï¼Œæœ€åä¸€å±‚çš„ hidden state ä½œä¸ºencoder æœ€ç»ˆè¾“å‡ºã€‚


### ğŸ„ MobileBERTLayer

```python
class MobileBertLayer(nn.Layer):
    def __init__(self, config):
        super().__init__()
        self.use_bottleneck = True
        self.num_feedforward_networks = 4

        self.attention = MobileBertAttention(config)
        self.intermediate = MobileBertIntermediate(config)
        self.output = MobileBertOutput(config)
        if self.use_bottleneck: self.bottleneck = Bottleneck(config)
        if config.num_feedforward_networks > 1:
            self.ffn = nn.LayerList([FFNLayer(config) for _ in range(config.num_feedforward_networks - 1)])
```

è¿™é‡Œè¦å‚è€ƒä¸Šé¢è®ºæ–‡é‡Œçš„å›¾ã€‚








### è®­ç»ƒç­–ç•¥












